==== Designing Scalable Systems ====

# Remember Basics
1M Kb = 1Gb
1B Kb = 1Tb

>> Java Primitives sizes:
char = 2 byte = 0 to 65K
byte = 8 bits = 2^8 (-128 to 127)
int = 4 byte = +-2B
long = 8 byte = +- 19 digits

>> Base64 encode/decode: https://en.wikipedia.org/wiki/Base64

>> Bloom filters: A DS to tell rapidly and memory-efficiently, whether an element is present in a set.
https://llimllib.github.io/bloomfilter-tutorial/

>> Choose the right way of Client side connection with server
https://medium.com/system-design-blog/long-polling-vs-websockets-vs-server-sent-events-c43ba96df7c1

=> TCP vs UDP:
 -> TCP characteristics:
    - is a connection-oriented protocol, w/ ACKs
    - it’s the most widely used protocol on the Internet
    - it guarantees that no packets are missing and all the data that’s sent makes it to the intended recipient
    - sends packets in order so they can be stitched back together easily.
    - it’s slower and requires more resources
    - has a bigger header than UDP
    - best suited for apps that need high reliability, and transmission time is relatively less critical.
 -> UDP characteristics:
    - is a connectionless protocol, No ACKs
    - is used for VoIP, video streaming, gaming and live broadcasts
    - it’s faster and needs fewer resources
    - the packets don’t necessarily arrive in order
    - it allows missing packets. The sender is unable to know whether a packet has been received
    - better suited for applications that need fast, efficient transmission, such as games.

=> Polling - Client polls looking for new content at regular intervals (Short & Long Poll)
=> Server Sent Event (SSE) - Server pushes new content to Client as it is available (Client subscribes to server events)
=> WebSocket: 2 way duplex communication appropriate for Chat/Messengers/Multiplayer Games/MobileApps
           -> Lightweight, no overhead of TCP handshake, always ON, realtime & event based, reactive w/ NodeJS impl
           -> https://stackoverflow.com/questions/14703627/websockets-protocol-vs-http
           -> https://www.quora.com/What-is-the-best-way-to-push-real-time-data-to-a-mobile-app-from-a-backend

>> Encryption:
  => Symmetric - Same public key b/w 2 parties is used to encrypt message
  => Asymmetric - Each party has a private & public key. Public key is shared with another party. Message are encrypted using the public key but can only be decrypted by private key.

>> Computing # servers: (How many servers for 1M reads/sec ? - Storage, CPU, I/O)
  => A commodity server (12 cores) can handle 100 concurrent threads / processes (after that it degrades)
  => Say Read or Write takes 100 ms, so 10 reads/sec per process = 100 threads x 10 reads = 1000 reads/sec
  => If we need 1M reads/sec = 1000 servers
  => Also, check I/O throughput. Say, each read takes 10KB data. A typical server supports 1 GB/sec = 100K reads/sec

========

# Cache Patterns:
  => https://www.ehcache.org/documentation/3.8/caching-patterns.html
  => https://dzone.com/articles/using-read-through-amp-write-through-in-distribute

-> Cache-aside: This is where application is responsible for reading and writing from the database and the cache doesn't
interact with the database at all. The cache is "kept aside" as a faster and more scalable in-memory data store.
The application checks the cache before reading anything from the database. And, the application updates the cache
after making any updates to the database. This way, the application ensures that the cache is kept synchronized with the database.

-> Read-through/Write-through (RT/WT): This is where the application treats cache as the main data store and reads data
from it and writes data to it. The cache is responsible for reading and writing this data to the database, thereby
relieving the application of this responsibility.

-> Write-behind lets your application quickly update the cache and return. Then, it lets the cache update the database in the background.

=======

# CAP Theorem:
When we design a distributed system, trading off among CAP (consistency, availability, and partition tolerance) is almost the first thing we want to consider.

1.Consistency: all nodes see the same data at the same time
2.Availability: a guarantee that every request receives a response about whether it succeeded or failed
3.Partition tolerance: system continues to operate despite arbitrary message loss or failure of part of the system

In a distributed context, the choice is between CP and AP. Unfortunately, CA is just a joke, because single point of failure is a red flag in the real distributed systems world.
To ensure consistency, there are some popular protocols to consider: 2PC, eventual consistency (vector clock + RWN), Paxos, In-Sync Replica, etc.

=======

# DB Patterns:
[SQL Rigid Schema]
0. RDBMS: Row-oriented Store,
   => MySQL, Oracle, Amazon RDS

[NoSQL Schema-less]
1. KV Store: Achieve an O(1) read/write performance on a fast and expensive media (like memory or SSD), instead of a traditional O(logn) read/write
   => Redis/Memcache (Redis supports data persistence while memcache does not).
   => Riak (supports secondary indexes with Solr, also has a Time Series optimized DB)
   => Amazon DynamoDB, Azure CosmosDB (supports relational queries)
2. Document Store: abstraction of a document store is like a KV store, but documents, like XML, JSON, BSON, and so on, are stored in the value part of the pair
   => MongoDB, CouchDB
3. Column-oriented Store: abstraction of a column-oriented store is like a giant nested map: ColumnFamily<RowKey, Columns<Name, Value, Timestamp>>
   => BigTable, Cassandra(derived from Google's BigTable), Druid, HBase
4. Graph Database: database’s abstraction is a graph. It allows us to store entities and the relationships between them.
   => GraphDB, Neo4J, Infinitegraph, FlockDB

[Hybrid]
5. Supports strong ACID, with horizontal scaling, replication, sharding etc & SQL support
   => RocksDB(embedded KV store) storage engine based - CockroachDB, Rocksandra, MongoRocks, MyRocks (FB uses it)

=======

# Real Time Streaming
  => Apply when looking for near real time aggregations / batch counts / insights / trends etc

=======
# Concerns to address while designing scalable systems:
1. Parallel Requests
2. Geo Location
3. Data Size
4. Single of point failure
5. Server Hotspot
6. Data Hotspot

# Building blocks (solves above concerns):
A. Data Replication (entire copy): 1,2,4
B. Data Sharding (data partitioning): 1,3,6 (some extent - if one row is hot spotting, it won't solve)
C. Caching (app layer, serving data): 1,2,6

# Design Patterns:
1. Micro-services: collection of loosely coupled systems, able to scale them independently (see pic)

# Storage Systems:
Logical Schema:
1. Relational
2. Key Value Pairs
3. Graph Model (eg. social graph)

Physical Schema (Disk):
1. Row major format (seeks all row contents on disk to even read a specific column)
   => Data stored as row contiguous form
   => Suited for OLTP (Online Transaction Processing - quick CRUD operations on PK of row)
2. Column family (easy access to particular columnar data)
   => Data stored as column contiguous form
   => Suited for OLAP (Online Analytical Processing - aggregations on huge column data sets)

Indexing:
Based on search criteria -
1. Hash Index (exact match)
2. n-ary B-tree Index (not exact; range based retrieval)
              n
          /   |   \
       [a-d] [e-g] [h-k]
3. Combination = Hash + B-tree (parts exact match, others range based)

# Sharding:
>> Horizontal Sharding [Most common pattern] - too many rows, partition by keys
>> Vertical Sharding - less rows but too big values (Netflix movie) - partition by value

1. Idea - 1 million users -> 100 shards * 10K users / shards
2. Goals - near even distribution of data, add new shards seamlessly, shards availability

  Approaches:
  1. Simplistic - fix n shards, shard = userId % n
     (+) Even distribution
     (-) Add new shards (formula changes, shard allocation changes, needs downtime for re-allocation)
     (-) Shards not available (can be solved though)

  2. Consistent Hashing -
     https://www.acodersjourney.com/system-design-interview-consistent-hashing/
     Say we come up with 0 to 100 hash range, we assign nodes A,B,C,D following hash ranges
               0       25
               A ----- B (0,25) => B takes 0,25 hash range, so any keys hashing to this range will go to this node
               |       |
               D ----- C
              75       50 (25,50)
     + Even distribution

     Adding a node - is splitting a range & seeding the new node with all the data that belongs to it:
               0       12
               A ----- X (0,12) => new node X takes 0,12 hash range
               |         \
               D --- C --- B (12,25)  => B has given up on some hash range
              75     50
         (50,75)     (25,50)
     + Adding new shards

     Shard replicas - apart from having a primary range, a node also has a secondary hash range that it stores
     Say, D stores not only (50,75), but also secondary for (25,50), hence we now have a replica in case of failures
     Note all shards replicas are also active-active
     + Shards availability

=======

# TEMPLATE For Solving SYSTEM DESIGN PROBLEMS
1. Requirements
2. Solve as single server, then scale out
3. Capacity Estimation
4. API Design
5. DB Schema
6. High Level Architecture Diagram
7. Detailed Design
  - Sharding
  - Replication
  - Caching
  - Rate Limiting
8. Load Balancing / Router / Aggregator
9. CDN (cache + blob storage) - Static images/videos/javascript caching on geographically distributed servers
                                to reduce web page load latency. Also called Edge server as it sits at Edge of a n/w
10. Security / Auth - Login service, DB (userId, passwd) + cache (userId, token).
                     Set token in session / cookie to validate subsequent client requests.
                     OAuth ?
11. Monitoring

=======

# Different Distributed Services patterns
  => Online / Realtime : Micro-services pattern
     -> Latency < 500 ms
  => Batch / Offline : Map Reduce pattern
     -> Large data-set, minutes to hours processing times, w/ per hour or per day frequency
  => In b/w or near real time : Stream Processing pattern
     -> NRT, micro-batching

=======

# Distributed Transactions
  => Avoid transactions in distributed systems
  => Use RDBMS
     -> may not scale
     -> Use same DB b/w microservices or replicate DB (eg. Order, InStock & Payments services) - defeats microservices pattern
  => 2 Phase commits - prepare & commit with rollbacks in case of failures
     -> Synchronous & slow
  => SAGA
     -> A microservice publishes change events to Kafka and another one listens to chain sequence, say Order followed by Payments
     -> Failures will be sent to a different Kafka for rolling back transactions.
     -> May become complex, use a orchestrator to co-ordinate sequences or do rollbacks - simpler
  => CQRS
     -> Separate Command which changes state of a system from Query which just does Reads
     -> Writes microservice will do complex transactions & publish Reads DTO out to Kafka to write it to Reads microservice.

=======

# LSM Trees (Log Structured Merge Trees)
  => Multi leveled trees L0 (smaller, in-memory), L1(merged from L1, stored on disk)
  => eg. BigTable, Cassandra, HBase, RocksDB












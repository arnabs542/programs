==== Designing Scalable Systems ====

# Remember Basics
1M Bytes = 1Mb
1M Kb / 1B Bytes = 1Gb
1B Kb / 1T Bytes = 1Tb

1 day has approx 100,000 sec

>> Java Primitives sizes:
boolean = ~1 byte
char = 2 byte = 0 to 65K  (1 byte for most common chars)
byte = 8 bits = 2^8 (-128 to 127)
int = 4 byte = +-2B
long = 8 byte = +- 19 digits
float = 4 bytes = 6 to 7 decimal point precision  (numbers represented as 1 bit for sign, 23 bits for mantissa, 8 bits for exponent eg. 1.23 * (10^)34)
double = 8 bytes = 15 to 16 decimal precision

>> Computing # servers: (How many servers for 1M reads/sec ? - Storage, CPU, I/O)
  => A commodity server (12 cores) can handle 100 concurrent threads / processes (after that it degrades)
  => Say Read or Write takes 100 ms, so 10 reads/sec per process = 100 threads x 10 reads = 1000 reads/sec
  => If we need 1M reads/sec = 1000 servers
  => Also, check I/O throughput. Say, each read takes 10KB data. A typical server supports 1 GB/sec = 100K reads/sec

>> How to represent very large numbers bigger than primitives MAX_VALUE?
  => Use BigInteger which basically stores each digit in int[]
  => For add/substract/multiply etc using carry overs & sign

>> Base64 encode/decode: https://en.wikipedia.org/wiki/Base64
 -> Uses chars A-Z,a-z,0-9,+,/ = 64 chars
 -> A=000000, B=000001... (6 bits per Base64 char)

>> Bloom filters: A DS to tell rapidly and memory-efficiently, whether an element is present in a set.
https://llimllib.github.io/bloomfilter-tutorial/

>> Choose the right way of Client side connection with server
https://medium.com/system-design-blog/long-polling-vs-websockets-vs-server-sent-events-c43ba96df7c1

=> How do web browser communicate over internet?
   - Type URL, browser lookup ISP provided DNS, IP routing over multiple gateways/routers, socket connect, TCP/IP communication - connect/handshake/ack,
   - Send request using HTTP GET,POST,PUT w/ headers content-type to webserver. Can be encrypted.
   - Webserver responds with 200 OK & response body sent as chunks via TCP. Webservices may use CDNs for media content.
   - Browser renders response which in turn can fire client ajax style TCP requests to render dynamic content async.
   - Browser uses cookies/memory/disk to cache/page/store meta info.

=> TCP vs UDP:
 -> TCP characteristics:
    - is a connection-oriented protocol, w/ ACKs
    - it’s the most widely used protocol on the Internet
    - it guarantees that no packets are missing and all the data that’s sent makes it to the intended recipient
    - sends packets in order so they can be stitched back together easily.
    - it’s slower and requires more resources
    - has a bigger header than UDP
    - best suited for apps that need high reliability, and transmission time is relatively less critical.
 -> UDP characteristics:
    - is a connectionless protocol, No ACKs
    - is used for VoIP, video streaming, gaming and live broadcasts
    - it’s faster and needs fewer resources
    - the packets don’t necessarily arrive in order
    - it allows missing packets. The sender is unable to know whether a packet has been received
    - better suited for applications that need fast, efficient transmission, such as games.

=> Polling - Client polls looking for new content at regular intervals (Short & Long Poll)
=> Server Sent Event (SSE) - Server pushes new content to Client as it is available (Client subscribes to server events)
=> WebSocket: 2 way duplex communication appropriate for Chat/Messengers/Multiplayer Games/MobileApps
           -> Lightweight, no overhead of TCP handshake, always ON, realtime & event based, reactive w/ NodeJS impl
           -> https://stackoverflow.com/questions/14703627/websockets-protocol-vs-http
           -> https://www.quora.com/What-is-the-best-way-to-push-real-time-data-to-a-mobile-app-from-a-backend

=> https://nordicapis.com/when-to-use-what-rest-graphql-webhooks-grpc/
   -> Go and gRPC have become a popular choice for microservices of late. gRPC is an efficient over the wire
      communication protocol. Built on top of http/2 and protobuf (in theory other serialization protocols can also be
      supported), this is supposed to be faster, with lesser network overhead and an efficient protocol. Protobufs are
      language and platform neutral systems used to serialize data, meaning that these communications can be efficiently serialized and communicated in an effective manner.
      Good for IoTs.

      Comparing Use Cases For REST, GraphQL, Webhooks, and gRPC
      REST: A stateless architecture for data transfer that is dependent on hypermedia. REST can tie together a wide range of resources that might be requested in a variety of formats for different purposes. REST is fundamentally concerned with stateless resource management, so it’s best used in such situations. Systems requiring rapid iteration and standardized HTTP verbiage will find REST best suited for their purposes.
      gRPC: A nimble and lightweight system for requesting data. gRPC, on the other hand, is best used when a system requires a set amount of data or processing routinely, and in which the requester is either low power or resource-jealous. The IoT is a great example of this.
      GraphQL: An approach wherein the user defines the expected data and format of that data. GraphQL is from Facebook, and that pedigree demonstrates its use case well — situations in which the requester needs the data in a specific format for a specific use. In those cases, those data formats and the relations between them are vitally important, and no other solution provides the same level of interconnected provision of data.
      Webhooks: Data updates to be served automatically, rather than requested. Finally, Webhooks are best used when the API in question primarily updates clients. While such APIs can also have other functions, even RESTful ones, the primary use of a Webhook microservice should be to update clients and provide updated, provisioned data upon the creation of the new, updated resource.

=> What is HTTP/2 ?
https://hackernoon.com/thoughts-on-grpc-c7ff8dcf8476
Http 2 is the next version of http protocol(http 1.1 is the most commonly used current version). Few points to note about http 2.0
-> It supports connection multiplexing. So if there are multiple parallel request from one server to another, all the responses can be sent together, interleaved, over a single connection, making it faster when compared to responses over separate connections.
-> Headers are compressed and transferred as binary data rather than plain text in http 1.1.
-> Connection reuse also means less overhead of ssl handshake. Only works with HTTPS hence more secure.

>> Encryption:
  => Symmetric - Same public key b/w 2 parties is used to encrypt message
  => Asymmetric - Each party has a private & public key. Public key is shared with another party. Message are encrypted using the public key but can only be decrypted by private key.

>> HTTP vs HTTPS : https://www.youtube.com/watch?v=hExRDVZHhig
  => "Hyper Text Transfer Protocol" - Former most widely used format of data exchange over web. Data is sent in clear
  text & susceptible to hacks, especially if it has sensitive data.
  => Secure HTTP - Data is encrypted & transferred over web which is impossible to hack. HTTPS secures data using one of the 2 protocols:
     1. SSL (Secure Socket Layer) - Uses public key encryption to secure data.
        Steps: Web browser connects to website & asks identify itself.
        Website sends a copy of it's SSL certificate. Browser authenticates the identity of website from CA & sends trusted OK msg to webserver.
        Webserver responds with an ACK so that ssl session can proceed. Encrypted data can now be exchanged over internet.
     2. TLS (Transport Layer Security) - SSL Successor & Latest industry standard cryptographic protocol.
        Based on SSL specs which Auths server, client & encrypts data. Most websites use HTTPS by default else Google
        flags them as "not secure" & penalizes them in search rankings.
  => More details in com.raj.scalablesystems.browserinternet.Browser_Communication

========

# Cache Patterns:
  => https://www.ehcache.org/documentation/3.8/caching-patterns.html
  => https://dzone.com/articles/using-read-through-amp-write-through-in-distribute

-> Cache-aside: This is where application is responsible for reading and writing from the database and the cache doesn't
interact with the database at all. The cache is "kept aside" as a faster and more scalable in-memory data store.
The application checks the cache before reading anything from the database. And, the application updates the cache
after making any updates to the database. This way, the application ensures that the cache is kept synchronized with the database.

-> Read-through/Write-through (RT/WT): This is where the application treats cache as the main data store and reads data
from it and writes data to it. The cache is responsible for reading and writing this data to the database, thereby
relieving the application of this responsibility.

-> Write-behind: lets your application quickly update the cache and return. Then, it lets the cache update the database in the background.

=======

# CAP Theorem:
When we design a distributed system, trading off among CAP (consistency, availability, and partition tolerance) is almost the first thing we want to consider.

1.Consistency: all nodes see the same data at the same time
2.Availability: a guarantee that every request receives a response about whether it succeeded or failed
3.Partition tolerance: system continues to operate despite arbitrary message loss or failure of part of the system

In a distributed context, the choice is between CP and AP. Unfortunately, CA is just a joke, because single point of failure is a red flag in the real distributed systems world.
To ensure consistency, there are some popular protocols to consider: 2PC, eventual consistency (vector clock + RWN), Paxos, In-Sync Replica, etc.

=======

# DB Patterns:
[SQL Rigid Schema]
0. RDBMS: Row-oriented Store,
   => MySQL, Oracle, Amazon RDS

[NoSQL Schema-less]
1. KV Store: Achieve an O(1) read/write performance on a fast and expensive media (like memory or SSD), instead of a traditional O(logn) read/write
   => Redis/Memcache (Redis supports data persistence while memcache does not).
   => Riak (supports secondary indexes with Solr, also has a Time Series optimized DB)
   => Amazon DynamoDB, Azure CosmosDB (supports relational queries)
2. Document Store: abstraction of a document store is like a KV store, but documents, like XML, JSON, BSON, and so on, are stored in the value part of the pair
   => MongoDB, CouchDB
3. Column-oriented Store: abstraction of a column-oriented store is like a giant nested map: ColumnFamily<RowKey, Columns<Name, Value, Timestamp>>
   => BigTable, Cassandra(derived from Google's BigTable), Druid, HBase
4. Graph Database: database’s abstraction is a graph. It allows us to store entities and the relationships between them.
   => GraphDB, Neo4J, Infinitegraph, FlockDB

[Hybrid]
5. Supports strong ACID, with horizontal scaling, replication, sharding etc & SQL support
   => RocksDB(embedded KV store) storage engine based - CockroachDB, Rocksandra, MongoRocks, MyRocks (FB uses it)

=======

# Real Time Streaming
  => Apply when looking for near real time aggregations / batch counts / insights / trends etc

=======
# Concerns to address while designing scalable systems:
1. Parallel Requests
2. Geo Location
3. Data Size
4. Single of point failure
5. Server Hotspot
6. Data Hotspot

# Building blocks (solves above concerns):
A. Data Replication (entire copy): 1,2,4
B. Data Sharding (data partitioning): 1,3,6 (some extent - if one row is hot spotting, it won't solve)
C. Caching (app layer, serving data): 1,2,6

# Design Patterns:
1. Micro-services: collection of loosely coupled systems, able to scale them independently (see pic)

# Storage Systems:
Logical Schema:
1. Relational
2. Key Value Pairs
3. Graph Model (eg. social graph)

Physical Schema (Disk):
1. Row major format (seeks all row contents on disk to even read a specific column)
   => Data stored as row contiguous form
   => Suited for OLTP (Online Transaction Processing - quick CRUD operations on PK of row)
2. Column family (easy access to particular columnar data)
   => Data stored as column contiguous form
   => Suited for OLAP (Online Analytical Processing - aggregations on huge column data sets)

Indexing:
Based on search criteria -
1. Hash Index (exact match)
2. BST (Self-balancing - Red/Black, AVL, B-Tree) - logn w/ balance cost
3. n-ary B-tree Index (not exact; range based retrieval). Refer to https://www.youtube.com/watch?v=aZjYr87r1b8
              n(d,g,k)
          /   |   \
       [a-d] [e-g] [h-k]      .... widely used in DB, keeps keys in sorted order for sequential traversing - https://en.wikipedia.org/wiki/B-tree
4. Combination = Hash + B-tree (parts exact match, others range based)

# Sharding:
>> Horizontal Sharding [Most common pattern] - too many rows, partition by keys
>> Vertical Sharding - less rows but too big values (Netflix movie) - partition by value

1. Idea - 1 million users -> 100 shards * 10K users / shards
2. Goals - near even distribution of data, add new shards seamlessly, shards availability

  Approaches:
  1. Simplistic - fix n shards, shard = userId % n
     (+) Even distribution
     (-) Add new shards (formula changes, shard allocation changes, needs downtime for re-allocation)
     (-) Shards not available (can be solved though)

  2. Consistent Hashing -
     https://www.acodersjourney.com/system-design-interview-consistent-hashing/
     Say we come up with 0 to 100 hash range, we assign nodes A,B,C,D following hash ranges
               0       25
               A ----- B (0,25) => B takes 0,25 hash range, so any keys hashing to this range will go to this node
               |       |
               D ----- C
              75       50 (25,50)
     + Even distribution

     Adding a node - is splitting a range & seeding the new node with all the data that belongs to it:
               0       12
               A ----- X (0,12) => new node X takes 0,12 hash range
               |         \
               D --- C --- B (12,25)  => B has given up on some hash range
              75     50
         (50,75)     (25,50)
     + Adding new shards

     Shard replicas - apart from having a primary range, a node also has a secondary hash range that it stores
     Say, D stores not only (50,75), but also secondary for (25,50), hence we now have a replica in case of failures
     Note all shards replicas are also active-active
     + Shards availability

=======

# TEMPLATE For Solving SYSTEM DESIGN PROBLEMS
1. Requirements
2. Solve as single server, then scale out
3. Capacity Estimation
4. API Design
5. DB Schema
6. High Level Architecture Diagram
7. Detailed Design
  - Sharding
  - Replication
  - Caching
  - Rate Limiting
8. Load Balancing / Router / Aggregator
9. CDN (cache + blob storage) - Static images/videos/javascript caching on geographically distributed servers
                                to reduce web page load latency. Also called Edge server as it sits at Edge of a n/w
10. Security / Auth - Login service, DB (userId, passwd) + cache (userId, token).
                     Set token in session / cookie to validate subsequent client requests.
                     OAuth ?
11. Monitoring

=======

# Different Distributed Services patterns
  => Online / Realtime : Micro-services pattern
     -> Latency < 500 ms
  => Batch / Offline : Map Reduce pattern
     -> Large data-set, minutes to hours processing times, w/ per hour or per day frequency
  => In b/w or near real time : Stream Processing pattern
     -> NRT, micro-batching

=======

# Distributed Transactions
  => Avoid transactions in distributed systems
  => Use RDBMS
     -> may not scale
     -> Use same DB b/w microservices or replicate DB (eg. Order, InStock & Payments services) - defeats microservices pattern
  => 2 Phase commits - prepare & commit with rollbacks in case of failures
     -> Synchronous & slow
  => SAGA
     -> A microservice publishes change events to Kafka and another one listens to chain sequence, say Order followed by Payments
     -> Failures will be sent to a different Kafka for rolling back transactions.
     -> May become complex, use a orchestrator to co-ordinate sequences or do rollbacks - simpler
  => CQRS
     -> Separate Command which changes state of a system from Query which just does Reads
     -> Writes microservice will do complex transactions & publish Reads DTO out to Kafka to write it to Reads microservice.

=======

# LSM Trees (Log Structured Merge Trees)
  => Multi leveled trees L0 (smaller, in-memory), L1(merged from L1, stored on disk)
  => eg. BigTable, Cassandra, HBase, RocksDB

# What is NGINX?
Nginx is a all in one web server which can also be used as a reverse proxy, load balancer, mail proxy and HTTP cache.









